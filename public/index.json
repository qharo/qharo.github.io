
[{"content":" While current state-of-the-art prompt compression methods rely on large neural networks or manual prompt engineering, this research proposes a lightweight alternative that leverages attention mechanisms inherent in transformer models. By extracting statistical features from attention vectors and training a Random Forest classifier, the approach achieves comparable performance to existing methods for a fraction of the inference cost and nearly no training cost. This makes the method particularly suitable for real-time context history compression in API-based LLM applications. Transformers # Transformer architecture revolutionized natural language processing through its self-attention mechanism, which allows tokens to dynamically influence each other\u0026rsquo;s representations. This attention mechanism computes relevance between queries and keys, applying the resulting weights to values, enabling the model to capture complex relationships and dependencies in text.\nIncreasing Computational Demand # This success has led to increasingly resource-intensive models, with sizes growing from BERT\u0026rsquo;s 110M parameters to GPT-3\u0026rsquo;s 175B and beyond. Context lengths have similarly expanded, from 512 tokens to over 1M tokens in models like Claude. This growth has significantly increased computational requirements, as attention mechanisms scale quadratically with context length.\nAPI Alternative # While this trend has pushed most organizations toward API-based solutions as a more convenient and cost-effective alternative to in-house development, it introduces new challenges around token usage, blackbox functioning and computational overhead.\nPrompt Compression # Prompt compression techniques fall into two main categories:\nAbstractive compression generates new, condensed versions of prompts, potentially using tokens not present in the original input. While this approach offers greater flexibility and potential for creative compression, it tends to be computationally intensive and risks semantic drift from the original content. Extractive compression, conversely, selects and retains tokens from the original input. Though more constrained in its approach, it offers faster computation and better preservation of original meaning, making it particularly suitable for maintaining semantic fidelity in compressed prompts. LLMLingua 1 # LLMLingua 1 introduced a two-step prompt compression framework based on perplexity optimization. The system first dynamically allocates compression ratios across prompt components (sentences, examples, etc.) based on user-defined targets. It then performs finer-grained optimization at the token level, leveraging the key insight that higher perplexity indicates more informative content. The framework utilized an Alpaca 7B model, requiring distribution alignment with target models, but demonstrated impressive results with up to 20x compression while maintaining prompt response quality.\nLLMLingua 2 # LLMLingua 2 addresses key limitations of its predecessor, particularly the reliance on a large aligned model and unidirectional attention constraints. It employs GPT-4 to create a high-quality token-labeled dataset, training a transformer encoder (xlm-roberta with approximately 355M parameters) for token classification. This approach achieved 3x-6x improvement in processing time while maintaining performance levels. The system has proven cost-effective, reducing both inference time and overall costs even when accounting for compression overhead, and has been successfully integrated into popular frameworks like LangChain and Prompt Flow.\nFramework # Transformer models derive their capabilities primarily from the attention mechanism, represented by the equation: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nWe hypothesized that token importance could be quantified through this attention structure. Our analysis focused specifically on attention vectors before the dot product operation, along the non-softmax axis, as this represents how much attention each token receives.\nThe initial challenge lay in efficiently processing the high-dimensional attention map space \\(R^{L \\times H \\times N \\times N }\\) where \\(L\\) is the number of layers, \\(H\\) is the number of heads and \\(N\\) is the number of tokens, which varies depending on the prompt.\nWe reframed the problem in 3 steps to effectively tackle it:\nLayerwise Processing: Considering one layer at a time, which reduced the target function to $$ f: \\mathbb{R}^{H \\times N \\times N} \\rightarrow [0, 1]^{N} $$\nIndividual Vector Analysis: As the attention mechanism continuously enhances the attention vectors with contextual information, we hypothesized that each attention vector could be used individually $$ f: \\mathbb{R}^{H \\times N} \\rightarrow [0, 1], \\quad \\forall i \\in {1, \\ldots, N} $$\nFeature Extraction: For each attention vector, we extract \\(f\\) statistical features that capture the key characteristics. This makes the input invariant to number fo tokens and finally leaves us with a problem framed as: $$ f: \\mathbb{R}^{f \\times N} \\rightarrow [0, 1], \\quad \\forall i \\in {1, \\ldots, N} $$\nThis final formulation served as the basis for our classification model.\nFeature Engineering Proposed Framework Experiments # Our framework required optimization of four key components:\nClassification Model\nAfter evaluating multiple approaches including Support Vector Machines (with RBF Kernel), Random Forest (100 trees), and a Neural Network (similar to LLMLingua 2\u0026rsquo;s token classification layer), Random Forest performed best. Feature Selection\nWe analyzed nine potential features: Mean, Standard Deviation, Median, Median Absolute Deviation (MAD), Self-Attention, Kurtosis, Skewness, Quartile Range Values and Entropy Through Recursive Feature Elimination, we identified four critical features: MAD, Entropy, Standard Deviation and Self-Attention Attention Model\nOur analysis revealed that fine-tuning played a less crucial role than initially anticipated, as vanilla XLM-RoBERTa showed performance comparable to LLMLingua 2\u0026rsquo;s fine-tuned model through the first five layers. This revealed that raw attention maps inherently contain sufficient information for effective compression, regardless of model fine-tuning. We ultimately selected the GTE model for its practical advantages: support for context lengths up to 8192 tokens and superior performance in our metrics. Dataset\nWe utilized the Microsoft MeetingBank-LLMCompressed dataset, which provided high-quality token-level compression labels. Final Framework\nOur implemented solution processes input text through the GTE transformer\u0026rsquo;s fifth layer, extracts our four identified features from the attention maps, and feeds these into a Random Forest classifier for token retention prediction. This streamlined pipeline offers an efficient balance between computational overhead and compression effectiveness.\nResults # Performance # We evaluated our model using LongBench, a comprehensive benchmark for testing long-context understanding capabilities. The benchmark consists of:\nSingle-document QA: Tasks requiring understanding of a single long document Multi-document QA: Tasks involving reasoning across multiple documents Summarization: Tasks focusing on condensing long documents Our Random Forest approach showed strong performance in single-document QA tasks, often outperforming LLMLingua 2, while remaining competitive in other categories. The benchmark tasks span from 2,000 to 18,000 tokens, providing a robust test of model capabilities across varying context lengths.\nTask LLMLingua Performance RF (ours) Performance Task Type Eval Metric Average Length Multifield QA 33.72 34.48 Single-doc QA F1 4,559 Qasper 31.17 33.86 Single-doc QA F1 3,619 Narrative QA 16.34 15.34 Single-doc QA F1 18,409 2wikimqa 35.83 34.71 Multi-doc QA F1 4,887 Hotpotqa 47.73 44.26 Multi-doc QA F1 9,151 MuSiQue 23.41 20.66 Multi-doc QA F1 11,214 Multi News 23.73 24.37 Summarization Rouge-L 2,113 Gov Report 25.12 22.48 Summarization Rouge-L 8,734 QM Sum 21.83 20.26 Summarization Rouge-L 10,614 Latency # We compared real-time processing speeds between LLMLingua 2 and the Random Forest method, tested on an Intel Xeon Platinum 8168 CPU. This significant speed difference demonstrates the Random Forest\u0026rsquo;s superior efficiency for CPU-based deployments, making it particularly suitable for real-time applications where GPU access might be limited or cost-prohibitive.\nNumber of Tokens LLMLingua 2 (s) RF (s) 500 tokens 7.74 1.45 2000 tokens 8.89 3.58 Our approach offers a resource-efficient solution to prompt compression by using a simple Random Forest classifier instead of fine-tuning large language models. Compared to LLMLingua 2\u0026rsquo;s 355M parameters and additional model layers, our method requires only 74M parameters by utilizing 5 transformer layers. This dramatically reduces both development and inference costs while extending context handling to 8192 tokens, effectively addressing the \u0026ldquo;lost in the middle\u0026rdquo; problem that affects many LLMs.\n","date":"1 September 2024","externalUrl":null,"permalink":"/experience/amadeus/","section":"Experience","summary":"Amadeus Research Team (ART)","title":"RF-Based Automatic Prompt Compression","type":"Experience"},{"content":" A PyTorch implementation of a Denoising Diffusion Probabilistic Model trained to generate Devanagari characters. It trains a 10 million parameter U-Net on 92,000 32x32 images. Denoising Diffusion Probabilistic Models # DDPMs are generative machine learning architectures that learn to replicate complex data distributions, allowing them to create new samples that share characteristics with their training data. Think of them as learning the \u0026ldquo;recipe\u0026rdquo; for creating data from scratch. The model operates through two complementary processes that form a Markov chain (where each state depends only on the immediately previous state):\nThe forward diffusion process methodically corrupts the original data by adding small amounts of Gaussian noise over many timesteps (T). This follows a carefully designed schedule that gradually transforms meaningful data (like a clear image) into pure random noise. The analogy of ink diffusing in water is apt - just as a sharp droplet slowly spreads until it\u0026rsquo;s uniformly distributed, the original data\u0026rsquo;s structure is systematically dissolved into randomness. The reverse diffusion process is where the magic of generation happens. Starting from pure noise (timestep T), the model applies what it learned during training to iteratively \u0026ldquo;denoise\u0026rdquo; the data. At each step counting down from T to 0, it: Identifies the noise component present in the current state Predicts how to partially remove it Produces a slightly cleaner version for the next step This step-by-step reconstruction gradually reveals coherent patterns that match the statistical properties of the training data. It\u0026rsquo;s like watching the ink diffusion process in reverse - random particles slowly coalescing into meaningful structure. The model learns this denoising process through training on many examples, effectively discovering how to traverse the path from noise to realistic data samples.\nDataset and Training # The model was trained using the Devanagari Handwritten Character Dataset from UCI Machine Learning Reposiroy. Training was performed on an NVIDIA RTX A4000 GPU (16GB VRAM) over 40 epochs.\nFor noise scheduling, I implemented the cosine variance schedule from Improved Denoising Diffusion Probabilistic Models, which improves upon linear and squared schedules by providing smoother timestep transitions and enhanced sample quality and training stability.\nReferences # Calvin Luo\u0026rsquo;s article beautifully explains the mathematics behind how diffusion models work. He provides an intuitive foundation, beginning with ELBO and tracing the evolution from variational autoencoders (VAEs) through Markovian hierarchical VAEs to modern diffusion models. The Explaining AI\u0026rsquo;s channel offers thorough coverage of both theory and implementation, with supporting code available in his GitHub repository. Improved Denoising Diffusion Probabilistic Models Denoising Diffusion Probabilistic Models qharo/Devanagiri-DDPM A PyTorch implementation of DDPM trained to generate Devanagiri characters. Python 0 0 ","date":"3 October 2024","externalUrl":null,"permalink":"/projects/ddpm/","section":"Projects","summary":"A PyTorch implementation of DDPM trained to generate Devanagiri Script.","title":"Devanagiri Diffusion Model","type":"Project"},{"content":" We focused on two main objectives:\nImplementing L1,âˆž projection in Convolutional Variational Autoencoders (CVAEs) to induce structural sparsity while preserving reconstruction quality Designing and implementing quaternary encoding schemes optimized for DNA sequence representation We applied these innovations to the Cool Chic model, achieving significant network sparsification (~80% for CAE, ~50% for MDC) while maintaining competitive rate-distortion performance.\nL1,âˆž Projection # L1,âˆž projection is a method for enforcing a structured budget constraint on neural network weights. Given a \u0026ldquo;budget\u0026rdquo; hyperparameter C, it first measures feature importance by finding the maximum absolute value within each column of the weight matrix. These column maximums are summed to quantify the total feature influence. When this sum exceeds our specified budget C, the projection algorithm activates, optimally redistributing the budget across features based on their relative importance:\nImportant features: Scaled down proportionally, preserving signs Less important features: Completely zeroed out This creates structured sparsity by removing entire features rather than individual weights, which differs from L1 regularization (random, scattered sparsity) and L1,1 projection (row-wise sample sparsity).\nQuaternary Encoding # DNA sequences consist of four nucleotide bases (Adenine, Thymine, Cytosine, Guanine), which naturally map to a base-4 numerical system (0, 1, 2, 3). While traditional neural networks typically operate in binary latent spaces, this biological constraint required us to develop a quaternary-formatted latent space. To achieve this, we implemented a modified Shannon-Fano encoding algorithm specifically optimized for base-4 representation. This adaptation necessitated changes to the entropy calculations, as they needed to account for four possible states rather than two.\nResults # Decoder Projection # Our initial experiment focused on inducing sparsity in the decoder network of a fully connected autoencoder using L1,âˆž projection. The study utilized metabolomic data from urine samples of Non-Small Cell Lung Cancer (NSCLC) patients, comprising of 469 NSCLC patients (pre-treatment), 536 control patients. Each sample contained 2,944 metabolomic features. While implementing L1,âˆž projection on the encoder network was relatively straightforward, the decoder presented unique challenges. The primary difficulty lay in determining the optimal direction of sparsity to achieve meaningful structural patterns without compromising the network\u0026rsquo;s performance.\nWithout Projection With Projection Convolutional VAE # Building on the success of the fully connected model, we extended our approach to image compression using a CVAE. This implementation aimed to achieve efficient image encoding and decoding while maintaining image quality.\nPSNR Comparison We implemented a quaternary encoding scheme to compress the latent space representation. Performance was measured in nats (natural units of information).\nComparing L1 Projection, Quaternary Projection Cool Chic # The final experiment focused on Multiple Description Coding (MDC) and Single Description Coding (SDC) using a sender-receiver architecture with the CoolChic model.\nPSNR vs Sparsity Model Weight Comparison When examining the network post-projection, we observed highly organized sparsity patterns emerging from what was previously a dense, interconnected structure. Despite this substantial reorganization, the network maintained its core functionality, successfully performing its intended tasks without significant performance degradation.\n","date":"1 October 2023","externalUrl":null,"permalink":"/experience/i3s/","section":"Experience","summary":"Centre National de la Recherche Scientifique (CNRS)","title":"L1,âˆž Convolutional-VAE Projection for DNA Encoding","type":"Experience"},{"content":" Projects Devanagiri Diffusion Model CV PyTorch A PyTorch implementation of DDPM trained to generate Devanagiri Script. Experience RF-Based Automatic Prompt Compression NLP PyTorch Internship Amadeus Research Team (ART) L1,âˆž Convolutional-VAE Projection for DNA Encoding CV PyTorch Internship Centre National de la Recherche Scientifique (CNRS) Education # EURECOM 2022-2024 Master\u0026#39;s in Data Science Relevant Coursework: Advanced Statistical Inference, Deep Learning, Distributed Systems and Cloud Computing Amrita Vishwa Vidyapeetham 2018-2022, GPA: 8.2 Bachelor\u0026#39;s in Technology (Electronics and Communication Engineering) Relevant Coursework: Pattern Recognition, Information Theory ","date":"3 October 2024","externalUrl":null,"permalink":"/","section":"Aditya Yogesh Nair","summary":"Projects Devanagiri Diffusion Model CV PyTorch A PyTorch implementation of DDPM trained to generate Devanagiri Script.","title":"Aditya Yogesh Nair","type":"page"},{"content":"","date":"3 October 2024","externalUrl":null,"permalink":"/tags/cv/","section":"Tags","summary":"","title":"CV","type":"tags"},{"content":"","date":"3 October 2024","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"3 October 2024","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch","type":"tags"},{"content":"","date":"3 October 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/experience/","section":"Experience","summary":"","title":"Experience","type":"experience"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/tags/internship/","section":"Tags","summary":"","title":"Internship","type":"tags"},{"content":"","date":"1 September 2024","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" This is a demo of the background layout. Switch layout \u0026orarr; npx blowfish-tools ","externalUrl":null,"permalink":"/home/","section":"Welcome to Blowfish! ðŸŽ‰","summary":"This is a demo of the background layout.","title":"Welcome to Blowfish! ðŸŽ‰","type":"home"}]